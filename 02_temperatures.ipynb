{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temperature dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numexpr\n",
    "import numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a plausible problem. We have a dataset of all daily temperatures measured at Newark since 1893 and we want to analyze it. First, this is the \"pure Python\" way to open it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "with open(\"data/newark-temperature-avg.txt\") as file:\n",
    "    temperatures = [float(line) for line in file]\n",
    "\n",
    "temperatures = np.array(temperatures)\n",
    "print(temperatures, len(temperatures), \"elements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Note:\n",
    ">\n",
    "> Don't forget the *double* percent sign for cell magics! Single percent sign is a line magic, which measures the line (basically nothing if you were trying to measure a cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could easily then convert this to NumPy. Let's instead use NumPy directly, which will save memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "temperatures = np.loadtxt(\"data/newark-temperature-avg.txt\")\n",
    "print(temperatures, len(temperatures), \"elements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly, this does not save time, since it's trying to be general and can support several things, like multiple columns and more. The reduction in time, clarity, and generality *should usually* be worth it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the rest of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_temperatures = np.loadtxt(\"data/newark-temperature-min.txt\")\n",
    "max_temperatures = np.loadtxt(\"data/newark-temperature-max.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the fraction of nan values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction_nan = np.sum(np.isnan(temperatures)) / len(temperatures)\n",
    "print(f\"Fraction of values that are NaN: {fraction_nan:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at two ways of doing the same thing: Computing missing temperatures from average of min and max temperatures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "missing = np.isnan(temperatures)\n",
    "imputed_temperatures = temperatures.copy()\n",
    "imputed_temperatures[missing] = 0.5 * (\n",
    "    min_temperatures[missing] + max_temperatures[missing]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "imputed_temperatures = np.where(\n",
    "    np.isnan(temperatures),  # condition\n",
    "    0.5 * (min_temperatures + max_temperatures),  # if true\n",
    "    temperatures,  # if false\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, timeit does not change the environment, so let's repeat that here. We will use np.mean, because it is more descriptive, even though it is slower. If we used `minmax_temps = np.stack([min_temperatures, max_temperatures])`, then it would be much closer in speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_temperatures = np.where(\n",
    "    np.isnan(temperatures),  # condition\n",
    "    np.mean([min_temperatures, max_temperatures], axis=0),  # if true\n",
    "    temperatures,  # if false\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction_nan = np.sum(np.isnan(imputed_temperatures)) / len(imputed_temperatures)\n",
    "print(f\"Fraction of values that are NaN: {fraction_nan:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try a more interesting calculation (we are limited in what we can find interesting to do here until we introduce Pandas, since it's a simple dataset).\n",
    "\n",
    "> #### Note:\n",
    "> \n",
    "> These are *very* simple calculations, but we can still see performance differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "c_temps = (imputed_temperatures - 32) * 5 / 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict: Will this be slower, faster, or the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "c_temps = (imputed_temperatures - 32) * (5 / 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On older NumPy, this used to be faster - due to fusion, it should be the same on Unix systems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "c_temps = imputed_temperatures - 32\n",
    "c_temps *= 5 / 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly, this is to simple to get help from numexpr:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "c_temps = numexpr.evaluate(\"(imputed_temperatures - 32) * (5/9)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even in this simple case, a properly compiled function can help out just a little:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.vectorize((numba.float64(numba.float64),), target=\"parallel\")\n",
    "def convert(degrees):\n",
    "    return (degrees - 32) * (5 / 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "c_temps = convert(imputed_temperatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas\n",
    "\n",
    "Let's try a little more analysis, but we will do it properly, in Pandas!\n",
    "\n",
    "The datasets above were really part of the newark-temperature csv file, so let's open that in Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = pd.read_csv(\n",
    "    \"data/newark-temperature.csv\",\n",
    "    index_col=\"DATE\",\n",
    "    usecols=\"DATE TAVG TMAX TMIN\".split(),\n",
    "    parse_dates=[\"DATE\"],\n",
    ")\n",
    "df_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fill in the NAN values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_orig.copy()\n",
    "df.TAVG[df.TAVG.isnull()] = df[df.TAVG.isnull()][[\"TMAX\", \"TMIN\"]].mean(axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, even better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_orig.copy()\n",
    "df.TAVG.where(~df.TAVG.isnull(), df[[\"TMAX\", \"TMIN\"]].mean(axis=1), inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better still:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_orig.copy()\n",
    "df.TAVG.fillna(df[[\"TMAX\", \"TMIN\"]].mean(axis=1), inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did the above calculations on a copy, so we could do them inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.TAVG.plot(style=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"1893-01-01\":\"1910-01-01\"].TAVG.plot(style=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm = df.groupby(pd.Grouper(freq=\"M\")).mean()\n",
    "dfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm[\"1893-01-01\":\"1920-01-01\"].TAVG.plot(style=\".-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing we can do is a rolling mean; let's average over three years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rolling(3 * 365).mean().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas: speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is not necessarily *faster* than raw NumPy. It is more descriptive and more powerful. When you need speed, ***profile*** it then write just what you need in numba or something similar.\n",
    "\n",
    "Here is the underlying array, as a PandasArray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm.TAVG.array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that a Series, the 1D array that makes up the columns of a DataFrame, actually stores two arrays; the data you see above and an index (reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This supports the Python 3 memoryview / NumPy array protocol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.asarray(dfm.TAVG.array)\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr.flags[\"OWNDATA\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So no copies are involved. You can now take full advantage of anything you could on a NumPy array. Note that if you want a numpy array, you can use the shortcut:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm.TAVG.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas: alternatives\n",
    "\n",
    "The Pandas DataFrame is wildly popular. So much so that it is being used as an API by projects that do things that normal Pandas does not do, such as out-of-memory DataFrames (Dask)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See also:\n",
    "\n",
    "* [CompClass: Structured data](https://nbviewer.jupyter.org/github/henryiii/compclass/blob/master/classes/week7/1_pandas.ipynb)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
